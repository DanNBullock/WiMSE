{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[align them in their goal to represent anatomy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whats in a voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**whats in a streamline**\n",
    "\n",
    "Before moving into a discussion of **how** our model of white matter anatomy is generated, we should consider an essential difference between the data representation of the T1 and the data reprsentation of white matter (i.e. tractography):\n",
    "\n",
    "T1 images and other nifti data objects represent the brain with voxel-value pairings, such that each volume of represented space is associated with a quantative measure of that space.  As an arbitrary example, within some spatial frame of reference (i.e. scanner space, ACPC space, etc) a measured volume represented in the nifti image's data field at coordinate (128,120, 80) could have value of 125.32.  Relatedly, this is in much the same fashion that the 2D images that we discussed previously represent each area of depicted space with a value (or in the case of a standard color image, 3 values corresponding to RGB values).  Our most common method for representing white matter, which we refer to as \"Tractography\", DOES NOT operate in this fashion.  To see why lets refer back to the table that was provided just before we began our consideration of jpegs\n",
    "\n",
    "|   | **Digital Photograph** | **Brain Image (T1)** | **Diffusion image** | **Tractography** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| _Object represented_ | visual scene | cranium / brain | cranium / brain | white matter of brain |\n",
    "| _Source system_ | camera | MRI scanner | MRI scanner | Mathematical model  | \n",
    "| _Source phenomena_ | reflected light | water / magnetic properties | water movement | orientation interpolation |\n",
    "| _Property of interest_ | topography | volumetric occupancy | tissue structure | putative axon collection traversal |\n",
    "| _File extension_ | .jpg, .png ... | .nifti, nii.gz | [dwi] .nifti, nii.gz | .fg, .trk, .tck |\n",
    "| _Metadata_ | exif | header | header | varies by format |\n",
    "| _Data size_ | 100s kb - 1s MB | ~2.5 - 5 MB | 50 MB - 1.5 GB |500 MB - 10 GB |\n",
    "| _Data dimensionality_ | &quot;2D&quot;(3 RGB layers) | 3D | 4D |1D nested? |\n",
    "| _Data &quot;atoms&quot;_ | pixels | voxels | voxel-angle |vectors (streamlines) |\n",
    "| _Data &quot;atom&quot; content_ | integer | float |float |ordered float sequence (nodes) |\n",
    "\n",
    "\n",
    "assign a quantative value to each of its constituitive elements (streamlines).  Instead they merely have positional/spatial characteristics.\n",
    "\n",
    "In a very general sense, the overarching goal of neuroimaging is to faithfully measure and represent the brain.  When we considered structural brain images (T1), we noted that this was not unlike the goal of standard photography, which attempts to capture the reflectance detail of a given scene.  In the case of neuroimaging though, we're doing this in 3D and grayscale.  This grayscale represents a quantative value associated with a particular volume of tissue.  While this is fine enough for capturing gross anatomical information (which would be useful in localizing strokes or brain lesions) it doesn't capture information that is specific to the essence of the gray and white matter.  To do this we need an imaging modality/capacity that asseses characteritics that are specific to that tissue:\n",
    "\n",
    "For **gray matter** we need to assess *neural activity*, for **white matter** we need to assess *axonal connectivity*.  These needs dictate the reprsentational elements we use.  With *activity*, voxels are still sufficient, though we consider them stretched across time.  With *connectivity* though, voxels are insufficient, and so we move to streamlines.\n",
    "\n",
    "**Lets consider what a streamline is for a moment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 142, 3]\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Path to the repo for this project\n",
    "repoPath='/Users/plab/Documents/ipynb/'\n",
    "\n",
    "# load a tractography file with a single streamline in it\n",
    "streamsObjIN=nib.streamlines.load(os.path.join(repoPath,'singleStream.tck'))\n",
    "\n",
    "# determine the number of streamlines\n",
    "print(list(np.shape(streamsObjIN.tractogram.streamlines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have loaded a .tck file (\"singleStream.tck\") and then printed out the dimensions of the object storing streamlines.  Here we see that it is 1 by 142 by 3.  This indicates that there is 1 streamline with 142 nodes across three dimensions (X, Y, and Z).  This is a fairly unique tractography file, in that it only has one streamline in it.  Typically, a tractography file will contain many thousands of streamlines.  In the case of a tractography file that is representing an entire brain's white matter, it will likely have millions of streamlines.  For now though, we'll focus on just one streamline to get a sense of what a streamline is.\n",
    "\n",
    "This streamline (like others) is an ordered sequence of nodes (points in three dimensional space) representing a path through the white matter of the brain.  **This should not be interpreted as representing a single axon**.  Rather, given the coarseness of our diffusion measure, and the necessarily inferential methods involved with generating these streamlines (we use our diffusion data to estimate where we should put the streamline), it would be better to interpret this as **\"our best guess as to where there appears to be a collection of axons continuously oriented in the same direction\"**.  The specifics of how these guesses are generated can be discussed elsewhere (within the context of tractography generation algorithms), but for now this description is sufficient to begin considering how streamlines and collections of streamlines represent the brain's white matter.  \n",
    "\n",
    "Lets plot this streamline below to see what one of these white matter representations looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niwidgets import StreamlineWidget\n",
    "from niwidgets.exampledata import streamlines\n",
    "\n",
    "sw = StreamlineWidget(filename=streamlines)\n",
    "style = {'axes': {'color': 'red',\n",
    "                  'label': {'color': 'white'},\n",
    "                  'ticklabel': {'color': 'white'},\n",
    "                  'visible': False},\n",
    "         'background-color': 'white',\n",
    "         'box': {'visible': False}}\n",
    "sw.plot(display_fraction=0.5, width=500, height=500, style=style, percentile=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the case of **gray matter**, it is the activity of the neurons that is of particular salience, however a static image cannot capture this.  As such, researchers often turn to functional MRI scanning to assess the activities of cortical tissue over a given period of time. In this way, the data structure for fMRI is extended over a 4th dimension, time.  This allows researchers to investigate how blood oxygen levels (**BOLD**, a proxy measure for neural activity) evolve over time, and associate these changes with experimental manipulations. However, the **white mater** does not contain neuron bodies, and thus typically does not exhibit the kind of biophysical processes that are measured by fMRI. Moreover, what we are intreseted in, relative to **white matter** is not it's *activity* but rather, its *connectivity*.  Just as fMRI does not measure brain/neuronal activity directly, the imaging modality used in the study of **white mater**, diffusion MRI, doesn't measure the characteristic of interest (axonal structure/connectivity) directly.  Instead, diffusion MRI measures the propensity of water to move in a specific direction.  Because **myelin** (the lipid-dense neural structure component which gives white matter it's lighter apperance) restricts water from escaping out of an axon, water thus tends to move in a direction that is perpindicular to the axon (in other words, along the length of the axon). \n",
    "\n",
    "Each of these more specialized approaches to investigating brain tissue requires a specialized processing method. In both cases, the processing is specific to a given voxel, as is the characteristic that is to be inferred from looking at these measures.  For fMRI, the temporal structure of the **BOLD** signal must be inferred from the changes observed between sampling time points (i.e. slices in the fMRI data object's 4th dimension).  For dMRI the axonal structure must be inferred from a multitude of measurements (taken at various angular orientations) of water's diffusion propensity.  Interestingly though, in order to explore connective characteristics of the white matter, this derived model of diffusion propensity must be used as the foundation for the generation of another model which attempts to encapsulate the gross axonal structure of the white matter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are attempting to use these brain images for research, it seems that we would want these to be of the highest resolution possble so that we can see the brain's constiuent components in in fine detail.  Given our previous look at nifti data stuctures, we know that the resolution is actually fairly coarse (~ 1mm^3).  But just for the sake of argument, lets think about how many voxels would it take to represent the brain on a cellular level.  \n",
    "\n",
    "**What sort of information would we need to make an informed estimate about this?**\n",
    "\n",
    "Well, first we would need an estimate of how many neurons are in the cerebral cortex. Herculano-Houzel notes in a 2009 review (https://doi.org/10.3389/neuro.09.031.2009) that a good estimate of this number is actually a recent development, and that most numbers cited were unsubstantiated.  The number provided by Herculano-Houzel is **16 billion** neurons in the cerebral cortex (though **69 billion** appear to be located in the cerebellum), and approximately **85 billion non-neuronal cells**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel dimensions for T1 (in mm)\n",
      "(1.25, 1.25, 1.25)\n",
      "\n",
      "Number of data occupied voxels\n",
      "972417\n"
     ]
    }
   ],
   "source": [
    "#preloading and processing\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "t1Path='/Users/plab/Documents/JupyerData/proj-5c50b6f12daf2e0032f880eb/sub-100206/dt-neuro-anat-t1w.tag-acpc_aligned.tag-brain_extracted.id-5c57072befbc2800526291bb/t1.nii.gz'\n",
    "img = nib.load(t1Path)\n",
    "print('Voxel dimensions for T1 (in mm)')\n",
    "voxelDims=img.header.get_zooms()\n",
    "print(voxelDims)\n",
    "print('')\n",
    "\n",
    "volData = img.get_fdata()\n",
    "unwrappedData=np.ndarray.flatten(volData)\n",
    "\n",
    "def largeVal(n):\n",
    "    return n>0\n",
    "\n",
    "result=map(largeVal,unwrappedData)\n",
    "largeBool=list(result)\n",
    "largeNum=sum(largeBool)\n",
    "print('Number of non zero (data containing) voxels')\n",
    "print(largeNum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number neurons\n",
      "100000000000\n",
      "\n",
      "Estimated number of glia\n",
      "1000000000000\n",
      "\n",
      "Number of cells per voxel (estimate)\n",
      "1131201.9431992653\n",
      "Number of cells per cubic mm (estimate)\n",
      "579175.3949180238\n",
      "\n",
      "Current data storage usage for T1\n",
      "3593891 bytes\n",
      "0.0033470718190073967 gigabytes\n",
      "\n",
      "Current data storage usage for T1\n",
      "8800000000000.0 bytes\n",
      "8195.638656616211 gigabytes\n"
     ]
    }
   ],
   "source": [
    "#lets assume all cells are layed out in a latice matrix (i.e. they are all orthogonal to each other and they can thus be represented as a matrix.)\n",
    "print('Estimated number neurons')\n",
    "#citation\n",
    "neuronNum=100000000000\n",
    "print(neuronNum)\n",
    "print('')\n",
    "\n",
    "print('Estimated number of glia')\n",
    "gliaNum=neuronNum*10\n",
    "print(gliaNum)\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#neuronBodySize=25*10^-6\n",
    "#gliaBodySize=15*10^-6\n",
    "\n",
    "#neuronVolume=[neuronBodySize^3]*neuronNum\n",
    "#gliaVolume=[gliaBodySize^3]*gliaNum\n",
    "\n",
    "totalCellNum=neuronNum+gliaNum\n",
    "\n",
    "print('Number of cells per voxel (estimate)')\n",
    "cellPerVox=totalCellNum/largeNum\n",
    "print(cellPerVox)\n",
    "\n",
    "\n",
    "print('Number of cells per cubic mm (estimate)')\n",
    "cellPerMM=cellPerVox/(voxelDims[0]*voxelDims[1]*voxelDims[2])\n",
    "print(cellPerMM)\n",
    "print('')\n",
    "\n",
    "axonNum=neuronNum\n",
    "\n",
    "print('Current data storage usage for T1')\n",
    "import os\n",
    "statinfo = os.stat(t1Path)\n",
    "T1bytes=statinfo.st_size\n",
    "print(f'{T1bytes} bytes' )\n",
    "T1gigabytes=T1bytes/1073741824\n",
    "print(f'{T1gigabytes} gigabytes' )\n",
    "print('')\n",
    "\n",
    "print('Storage for a ')\n",
    "percellT1bytes=cellPerVox*largeNum*8\n",
    "print(f'{percellT1bytes} bytes' )\n",
    "percellT1gigabytes=percellT1bytes/1073741824\n",
    "print(f'{percellT1gigabytes} gigabytes' )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

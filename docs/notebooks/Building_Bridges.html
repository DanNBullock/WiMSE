
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Building bridges from the familiar to the unfamiliar &#8212; White Matter Segmentation Education (WiMSE)</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to discritized image representation and maps" href="Intro_to_discretized_image_representation_%26_maps.html" />
    <link rel="prev" title="White Matter Segmentation Education - WiMSE" href="../chapterSummaries.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">White Matter Segmentation Education (WiMSE)</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../landingPage.html">
   White Matter Segmentation Education (WiMSE)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Front matter
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Author_and_funding_information.html">
   Author and Funding information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapterSummaries.html">
   White Matter Segmentation Education - WiMSE
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Building bridges from the familiar to the unfamiliar
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Building intuitions with digital images
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro_to_discretized_image_representation_%26_maps.html">
   Introduction to discritized image representation and maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Aligning_two_images.html">
   Aligning two images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Multi_object_maps_in_images.html">
   Multi object maps in images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="A_consideration_of_jpegs_and_brain_images.html">
   A consideration of jpegs and brain images
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Working with NIfTI data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="How_to_represent_the_brain%27s_anatomy_-_as_a_volume.html">
   How to represent the brain’s anatomy - as a volume
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="A_quick_demonstration_of_linear_affine_transformations_in_3-D.html">
   A quick demonstration of linear affine transformations in 3-D
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="How_to_interpret_a_volumetric_brain_segmentation.html">
   How to interpret a volumetric brain segmentation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  White matter and tractography
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Highways_of_the_brain.html">
   Highways of the brain
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="The_measurement%2C_the_object%2C_and_the_modality_-_What%27s_measured_in_diffusion_imaging.html">
   The measurement, the object, and the modality - Whats in measured in diffusion imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="The_voxel_and_the_streamline.html">
   The voxel and the streamline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="The_source_tractogram.html">
   The source tractogram
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Segmenting tractography
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="A_first_segmentation.html">
   A first segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Biological_Plausibility_for_Tractograms.html">
   Biological Plausibility for Tractograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Segmenting_Tracts-Conceptualy.html">
   Segmenting Tracts - Conceptualy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Segmenting_Tracts-Conceptualy.html#why-segment-a-tract">
   Why segment a tract
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ROIs_as_tools.html">
   ROIs as tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Using_ROIs_as_tools.html">
   <em>
    Using
   </em>
   ROIs as tools
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/Building_Bridges.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/DanNBullock/WiMSE/edit/master/notebooks/Building_Bridges.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/DanNBullock/WiMSE/master?urlpath=tree/notebooks/Building_Bridges.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#but-why-jpegs">
   But… why JPEGs?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-intuitions-from-the-familiar">
     Building intuitions from the familiar
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-reassurance-to-the-initiated">
     A reassurance to the initiated
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-big-picture-a-philosophy-to-segmentation">
   The big picture - A philosophy to segmentation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-analogy-table">
     The analogy table
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-terms-in-the-analogy-table">
     Defining terms in the analogy table
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-demonstration-of-the-strength-of-the-analogy">
   A demonstration of the strength of the analogy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-to-make-of-the-madlib">
   What to make of the Madlib
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#at-the-heart-of-it-all">
   At the heart of it all
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-s-a-meta-ontology-for-that-matter-what-s-an-ontology">
     What’s a meta-ontology?  For that matter, what’s an ontology?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-second-insight-from-the-analogy-framework-the-role-of-representation">
     A second insight from the analogy framework: the role of representation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-we-care-about-image-representations-generally-and-specifically">
   Why we care about image representations generally… and specifically
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-rough-attempt-at-a-semantics-for-digital-white-matter">
   A rough attempt at a semantics for digital white matter
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-grand-insight">
   The grand insight
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="building-bridges-from-the-familiar-to-the-unfamiliar">
<h1>Building bridges from the familiar to the unfamiliar<a class="headerlink" href="#building-bridges-from-the-familiar-to-the-unfamiliar" title="Permalink to this headline">¶</a></h1>
<div class="section" id="but-why-jpegs">
<h2>But… why JPEGs?<a class="headerlink" href="#but-why-jpegs" title="Permalink to this headline">¶</a></h2>
<p>Although the title of this collection of interactive lessons is WiMSE (White Matter Segmentation Education), and presumably you are here to learn about neuroscience, our first several lessons will have us exploring JPEGs rather than brain images.  <strong>Why is that</strong>?</p>
<div class="section" id="building-intuitions-from-the-familiar">
<h3>Building intuitions from the familiar<a class="headerlink" href="#building-intuitions-from-the-familiar" title="Permalink to this headline">¶</a></h3>
<p>Although our ultimate goal will be to develop an intuitive understanding of how to conduct digital investigations of white matter anatomy, it will be helpful to begin with an example of a data structure that everyone (at least, denizens of our modern digital era) is familar with.  This is where JPEGs come in.  <a class="reference external" href="https://jpeg.org/jpeg/">JPEGs (Joint Photographic Experts Group)</a> are a standardized two dimensional (2D) digital image format that we typicaly encounter dozens of time a day as we browse the internet.  There are other image formats (PNG, for example), but JPEG is perhaps the most famous for images.  As such, we will begin exploring concepts related to structured data representation (and im particular, <em>image</em> represenatation) using JPEGs.  We will use JPEGs to establish a number of analogies to the primary image type used in neuroimaging research, the <a class="reference external" href="https://nifti.nimh.nih.gov/">NIfTI (Neuroimaging Informatics Technology Initiative)</a>.  Research has shown that analogies and metaphors are particularly helpful when learning new concepts (REINDERS DUIT, 1991).  Indeed, in many cases the comparisons we make will be more akin to logical extensions than metaphors.  Regardless, it is <strong>not</strong> expected that individuals using this lesson set are particularly familiar with the NIfTI data format, and so several non-technical features of this data type will be be discussed to help build this understanding.</p>
</div>
<div class="section" id="a-reassurance-to-the-initiated">
<h3>A reassurance to the initiated<a class="headerlink" href="#a-reassurance-to-the-initiated" title="Permalink to this headline">¶</a></h3>
<p>For more experienced users though, our upcoming discussion of the NIfTI format may seem out of place.   NIfTI data objects aren’t used to store connectivity data, which is the essence of white matter (and the key feature that data formats for white matter attempt to capture).  While this is true, it is necesary to discuss NIfTI data structures for two important reasons:  (1)  The difficulties in segmenting white matter structures (in connectivity-centric data formats) are well highlighted by comparsion to the process of segmentation volumetric structures (in NIfTI/volumetric-type formats) and (2) A good white matter segmentation can make exentsive use of information stored in or derived from NIfTI files (for example, <a class="reference external" href="https://surfer.nmr.mgh.harvard.edu/">FreeSurfer</a> parcellations).  Thus, our consideration of white matter segmentation will begin with digital images, proceed to NIfTI-type images (in this case T1 images), and then move on to the topics of tractography and segmentation.</p>
</div>
</div>
<div class="section" id="the-big-picture-a-philosophy-to-segmentation">
<h2>The big picture - A philosophy to segmentation<a class="headerlink" href="#the-big-picture-a-philosophy-to-segmentation" title="Permalink to this headline">¶</a></h2>
<p>Below you’ll find a table that provides an outline of the oveararching analogies between the various data formats that we will encounter in the following lessons.  As we proceed through the chapters we’ll repeatedly find ourselves investigating concepts with relatively familiar types of data (even for those who don’t do much programming or data processing), and then seeing how those same concepts can be applied to relatively unfamilar types of data.  At various points we will return to a consideration of this table and reflect on what it is about the current data type/object.</p>
<div class="section" id="the-analogy-table">
<h3>The analogy table<a class="headerlink" href="#the-analogy-table" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Digital Photography</strong></p></th>
<th class="head"><p><strong>Structural Brain Imaging (T1)</strong></p></th>
<th class="head"><p><strong>Diffusion Imaging (DWI)</strong></p></th>
<th class="head"><p><strong>Tractography</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Data Token</em></p></td>
<td><p>digital photo image</p></td>
<td><p>structural brain image (T1)</p></td>
<td><p>diffusion image (DWI)</p></td>
<td><p>tractogram</p></td>
</tr>
<tr class="row-odd"><td><p><em>Object represented</em></p></td>
<td><p>visual scene</p></td>
<td><p>cranium / brain</p></td>
<td><p>cranium / brain</p></td>
<td><p>white matter of brain</p></td>
</tr>
<tr class="row-even"><td><p><em>Source system</em></p></td>
<td><p>camera</p></td>
<td><p>MRI scanner</p></td>
<td><p>MRI scanner</p></td>
<td><p>Mathematical model</p></td>
</tr>
<tr class="row-odd"><td><p><em>Source phenomena</em></p></td>
<td><p>reflected light</p></td>
<td><p>water / magnetic properties</p></td>
<td><p>water movement</p></td>
<td><p>orientation interpolation</p></td>
</tr>
<tr class="row-even"><td><p><em>Property of interest</em></p></td>
<td><p>topography</p></td>
<td><p>volumetric occupancy</p></td>
<td><p>tissue structure</p></td>
<td><p>putative axon collection traversal</p></td>
</tr>
<tr class="row-odd"><td><p><em>File extension</em></p></td>
<td><p>.jpg, .png …</p></td>
<td><p>.nifti, nii.gz</p></td>
<td><p>(dwi) .nifti, nii.gz</p></td>
<td><p>.fg, .trk, .tck</p></td>
</tr>
<tr class="row-even"><td><p><em>Metadata</em></p></td>
<td><p>exif</p></td>
<td><p>header</p></td>
<td><p>header</p></td>
<td><p>varies by format</p></td>
</tr>
<tr class="row-odd"><td><p><em>Data size</em></p></td>
<td><p>100s kb - 1s MB</p></td>
<td><p>~2.5 - 5 MB</p></td>
<td><p>50 MB - 1.5 GB</p></td>
<td><p>500 MB - 10 GB</p></td>
</tr>
<tr class="row-even"><td><p><em>Data dimensionality</em></p></td>
<td><p>“2D”(3 RGB layers)</p></td>
<td><p>3D</p></td>
<td><p>4D</p></td>
<td><p>1D nested?</p></td>
</tr>
<tr class="row-odd"><td><p><em>Data “atoms”</em></p></td>
<td><p>pixels</p></td>
<td><p>voxels</p></td>
<td><p>voxel-angle</p></td>
<td><p>vectors (streamlines)</p></td>
</tr>
<tr class="row-even"><td><p><em>Data “atom” content</em></p></td>
<td><p>integer</p></td>
<td><p>float</p></td>
<td><p>float</p></td>
<td><p>ordered float sequence (nodes)</p></td>
</tr>
<tr class="row-odd"><td><p><em>Implicit features of interest</em></p></td>
<td><p>object differences</p></td>
<td><p>tissue differences</p></td>
<td><p>tissue differences</p></td>
<td><p>gross white matter / informational connectivity</p></td>
</tr>
<tr class="row-even"><td><p><em>Implicit categories</em></p></td>
<td><p>ordinary, every day objects</p></td>
<td><p>brain regions</p></td>
<td><p>brain regions</p></td>
<td><p>tracts and information streams</p></td>
</tr>
<tr class="row-odd"><td><p><em>Atom-object correspondence</em></p></td>
<td><p>isometric visual angle unit</p></td>
<td><p>sub volume of brain</p></td>
<td><p>sub volume of brain, measured from a specific angle</p></td>
<td><p>(putative)  axon collection traversal</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="defining-terms-in-the-analogy-table">
<h3>Defining terms in the analogy table<a class="headerlink" href="#defining-terms-in-the-analogy-table" title="Permalink to this headline">¶</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Term</strong></p></th>
<th class="head"><p><strong>Rough definition</strong></p></th>
<th class="head"><p><strong>Source or representation characteristic</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Data Token</em></p></td>
<td><p>A particular instance of a data type</p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-odd"><td><p><em>Object represented</em></p></td>
<td><p>The (real world) thing which the <em>Data Token</em> contains information about</p></td>
<td><p>Source</p></td>
</tr>
<tr class="row-even"><td><p><em>Source system</em></p></td>
<td><p>The technological/methodological system used to generate the data contained within a  <em>Data Token</em></p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-odd"><td><p><em>Source phenomena</em></p></td>
<td><p>The general kind of quantative property of the <em>Source system</em> makes measurements of</p></td>
<td><p>Source</p></td>
</tr>
<tr class="row-even"><td><p><em>Property of interest</em></p></td>
<td><p>The more qualatative feature of the <em>Object represented</em> that the <em>Source system</em> is understood to capture (via the <em>Source phenomena</em> proxy</p></td>
<td><p>Source</p></td>
</tr>
<tr class="row-odd"><td><p><em>File extension</em></p></td>
<td><p>File extensions typically associated</p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-even"><td><p><em>Metadata</em></p></td>
<td><p>Method for storing infomation about the manner in which the information was derived</p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-odd"><td><p><em>Data size</em></p></td>
<td><p>Typical size of the associated <em>Data Token</em></p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-even"><td><p><em>Data dimensionality</em></p></td>
<td><p>The dimensionality of the <em>Data Token</em></p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-odd"><td><p><em>Data “atoms”</em></p></td>
<td><p>The atomistic data components composing the <em>Data Token</em></p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-even"><td><p><em>Data “atom” content</em></p></td>
<td><p>The specific content/format of the <em>Data “atoms”</em></p></td>
<td><p>Representation</p></td>
</tr>
<tr class="row-odd"><td><p><em>Implicit features of interest</em></p></td>
<td><p>The (implicit) feature of the <em>Object represented</em> that the <em>Data Token</em> is suppose to faithfully capture.  Failure to capture these features would constitute a fundemntal failure for the <em>Data Token</em>.</p></td>
<td><p>Source</p></td>
</tr>
<tr class="row-even"><td><p><em>Implicit categories</em></p></td>
<td><p>Theoretically, the kinds of categorical delineations that could/would be made using effective leveraging of information about the <em>Implicit features of interest</em></p></td>
<td><p>Source</p></td>
</tr>
<tr class="row-odd"><td><p><em>Atom-object correspondence</em></p></td>
<td><p>The “real world” entities that the <em>Data “atoms”</em> are supposed to correspond to</p></td>
<td><p>Source</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="a-demonstration-of-the-strength-of-the-analogy">
<h2>A demonstration of the strength of the analogy<a class="headerlink" href="#a-demonstration-of-the-strength-of-the-analogy" title="Permalink to this headline">¶</a></h2>
<p>A good deal of time and effort has been taken to present and characterize the analogy between the various imaging modalities we will consider.  This is because the analogy is taken to be both (1) fundamental and (2) strong.  To demonstrate the strenght of this analogy, we’ll next make use of what is, in essence, a madlib.  Specifically, because of how well these modalities align with one another, and how carefully the characteristics have been operationalized, it will be shown that it is possible to generate a standard description of all modalities, that has the specifics of each modality populated from the table above.  Despite being “form generated” in this sense, it will nonetheless be found to be both informative and insightful.</p>
<p>After running the code block below, use the slider to slide between the columns of the analogy table to read a comprehensive characterization of the relevant features of each particular imaging modality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#this code ensures that we can navigate the WiMSE repo across multiple systems</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#get top directory path of the current git repository, under the presumption that </span>
<span class="c1">#the notebook was launched from within the repo directory</span>
<span class="n">gitRepoPath</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s1">&#39;git&#39;</span><span class="p">,</span> <span class="s1">&#39;rev-parse&#39;</span><span class="p">,</span> <span class="s1">&#39;--show-toplevel&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1">#move to the top of the directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">gitRepoPath</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#get the path to the saved csv version of the analogy table</span>
<span class="n">analogyTablePath</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gitRepoPath</span><span class="p">,</span><span class="s1">&#39;exampleData&#39;</span><span class="p">,</span><span class="s1">&#39;analogyTable.csv&#39;</span><span class="p">)</span>

<span class="n">analogyTable</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">analogyTablePath</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">#establish a bolding and coloring method up front</span>
<span class="c1">#taken from https://stackoverflow.com/questions/8924173/how-do-i-print-bold-text-in-python</span>
<span class="k">class</span> <span class="nc">color</span><span class="p">:</span>
   <span class="n">PURPLE</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[95m&#39;</span>
   <span class="n">CYAN</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[96m&#39;</span>
   <span class="n">DARKCYAN</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[36m&#39;</span>
   <span class="n">BLUE</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[94m&#39;</span>
   <span class="n">GREEN</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[92m&#39;</span>
   <span class="n">YELLOW</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[93m&#39;</span>
   <span class="n">RED</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[91m&#39;</span>
   <span class="n">BOLD</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[1m&#39;</span>
   <span class="n">UNDERLINE</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[4m&#39;</span>
   <span class="n">END</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\033</span><span class="s1">[0m&#39;</span>

<span class="n">columnIndex</span><span class="o">=</span><span class="mi">0</span>
<span class="c1">#update the printout     </span>
<span class="c1">#def generateAnalogyMadlib(columnIndex):</span>

<span class="c1">#create bold-green and double-end text holder variables to make the following lines shorter</span>
<span class="n">BG</span><span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">BOLD</span> <span class="o">+</span> <span class="n">color</span><span class="o">.</span><span class="n">PURPLE</span> <span class="o">+</span> <span class="n">color</span><span class="o">.</span><span class="n">UNDERLINE</span>
<span class="n">DE</span><span class="o">=</span> <span class="n">color</span><span class="o">.</span><span class="n">END</span> <span class="o">+</span> <span class="n">color</span><span class="o">.</span><span class="n">END</span> <span class="o">+</span> <span class="n">color</span><span class="o">.</span><span class="n">END</span>

<span class="c1">#create function for madlib generation</span>
<span class="k">def</span> <span class="nf">generateMadlib</span><span class="p">(</span><span class="n">currentColumn</span><span class="p">):</span>

    <span class="c1">#doing it this way to (1) save space later and (2) incase the table changes around later</span>
    <span class="c1">#inelegant, but transparent</span>
    <span class="c1">#will cause problems if the entries in the first column of the saved analogies table csv is altered</span>
    <span class="n">curModality</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;General imaging modality&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curDToken</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Data token&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curObjectR</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Object represented&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curSourceSys</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Source system&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curSourcePhe</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Source phenomena&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curPropOfInt</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Property of interest&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curFileExt</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;File extension&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curMetaDat</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Metadata&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curDataSz</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Data size&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curDataDim</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Data dimensionality&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curDataAtm</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Data atom&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curDataAtmCont</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Data atom content&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curImpInt</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Implicit features of interest&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curImpCat</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Implicit categories&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>
    <span class="n">curAtmObjCorr</span><span class="o">=</span><span class="n">BG</span> <span class="o">+</span> <span class="n">analogyTable</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">analogyTable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Atom-object correspondence&#39;</span><span class="p">,</span><span class="n">currentColumn</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">DE</span>

    <span class="c1">#need to play with wraparound settings for this</span>
    <span class="n">textToPrint</span><span class="o">=</span><span class="s1">&#39;A fill-in-the-blank description of &#39;</span> <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> <span class="s1">&#39; :</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;When we engage in research using &#39;</span> <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> <span class="s1">&#39; we are interested in learning more about a particular &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> \
        <span class="s1">&#39;.  However, in order to study the given &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> <span class="s1">&#39; effectively, we need to preserve information (i.e. observations) about it for later analysis.&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  In the case of &#39;</span> <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> <span class="s1">&#39; we typically use a(n) &#39;</span> <span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span> <span class="s1">&#39; to obtain data about a &#39;</span> <span class="o">+</span><span class="n">curObjectR</span> <span class="o">+</span><span class="s1">&#39;. It is worth noting though, that our decision to use a &#39;</span>  <span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span> \
        <span class="s1">&#39; is based upon (1) the availability of viable alternatives and (2) the general property of the &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> <span class="s1">&#39; (e.g. &#39;</span> <span class="o">+</span> <span class="n">curPropOfInt</span> <span class="o">+</span> <span class="s1">&#39;)&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39; that we are interested in, or whatever more specific goals we may have.  Indeed, the kinds of scientific questions we can ask about &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> <span class="s1">&#39;s &#39;</span> <span class="o">+</span> \
        <span class="s1">&#39; (or  &#39;</span> <span class="o">+</span><span class="n">curImpInt</span><span class="o">+</span> <span class="s1">&#39;, or even &#39;</span> <span class="o">+</span> <span class="n">curImpCat</span> <span class="o">+</span> <span class="s1">&#39;) using &#39;</span> <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> \
        <span class="s1">&#39; are delimited by the technical limitations of the&#39;</span> <span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span> <span class="s1">&#39;s used,&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39; and so to then, are the general kinds of things that end up getting studied/characterized using &#39;</span><span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> \
        <span class="s1">&#39;.  Resulting chicken-or-the-egg conundrums and streetlight effects notwithstanding, the ability to develop insights about &#39;</span>  <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> <span class="s1">&#39;s &#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;using information about &#39;</span> <span class="o">+</span> <span class="n">curPropOfInt</span> <span class="o">+</span> <span class="s1">&#39; derived from &#39;</span> <span class="o">+</span> <span class="n">curSourcePhe</span> <span class="o">+</span> <span class="s1">&#39; has, in practice, proven to be useful for identifying &#39;</span> <span class="o">+</span> <span class="n">curImpInt</span> <span class="o">+</span> \
        <span class="s1">&#39;, and (by extension) &#39;</span> <span class="o">+</span> <span class="n">curImpCat</span> <span class="o">+</span> <span class="s1">&#39;, both in specific &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span><span class="s1">&#39;s and in &#39;</span> <span class="o">+</span> <span class="n">curObjectR</span> <span class="o">+</span> <span class="s1">&#39;s generally.&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  Lets briefly consider how the data objects used in &#39;</span> <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> <span class="s1">&#39; facilitate this.</span><span class="se">\n\n</span><span class="s1">&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  The scientific utility of &#39;</span>  <span class="o">+</span> <span class="n">curModality</span> <span class="o">+</span> <span class="s1">&#39; is underwritten by our ability to store, access, and modify information &#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;contained within the standard data object used in this disipline, the &#39;</span> <span class="o">+</span> <span class="n">curDToken</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  In each of these data objects information that was produced by a &#39;</span><span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span><span class="s1">&#39; measuring &#39;</span> <span class="o">+</span> <span class="n">curSourcePhe</span> <span class="o">+</span> <span class="s1">&#39; is stored in a structured fashion.&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  Specifically, in the case of a &#39;</span>  <span class="o">+</span> <span class="n">curDToken</span> <span class="o">+</span> <span class="s1">&#39;, that information is stored in a &#39;</span> <span class="o">+</span> <span class="n">curDataDim</span> <span class="o">+</span> <span class="s1">&#39; structure &#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;wherein each &#39;</span> <span class="o">+</span> <span class="n">curDataAtm</span> <span class="o">+</span> <span class="s1">&#39; contains a &#39;</span> <span class="o">+</span> <span class="n">curDataAtmCont</span> <span class="o">+</span> <span class="s1">&#39;.  Each of these &#39;</span> <span class="o">+</span> <span class="n">curDataAtm</span> <span class="o">+</span> <span class="s1">&#39;s stores information about &#39;</span> <span class="o">+</span> <span class="n">curSourcePhe</span> <span class="o">+</span> \
        <span class="s1">&#39; as measured by the source &#39;</span> <span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span> <span class="s1">&#39; and, in doing so, represents a(n) &#39;</span> <span class="o">+</span> <span class="n">curAtmObjCorr</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;  Typically, a &#39;</span> <span class="o">+</span> <span class="n">curDToken</span> <span class="o">+</span> <span class="s1">&#39; contains a(n) &#39;</span> <span class="o">+</span> <span class="n">curMetaDat</span> <span class="o">+</span> <span class="s1">&#39; component which stores information about the source &#39;</span> <span class="o">+</span> <span class="n">curSourceSys</span> <span class="o">+</span> \
        <span class="s1">&#39; or other information relevant to (but not derivable from) the data object itself.  These files/data objects are typically associated with file extensions like &#39;</span> <span class="o">+</span> \
         <span class="n">curFileExt</span> <span class="o">+</span> <span class="s1">&#39; and are about &#39;</span> <span class="o">+</span> <span class="n">curDataSz</span> <span class="o">+</span> <span class="s1">&#39; in size, which can give you some sense of &#39;</span> <span class="o">+</span> \
        <span class="s1">&#39;the number of &#39;</span> <span class="o">+</span> <span class="n">curDataAtmCont</span> <span class="o">+</span> <span class="s1">&#39;s the object contains.&#39;</span>
    
    <span class="c1">#add brackets for the 0, i.e. label column, case</span>
    <span class="k">if</span> <span class="n">currentColumn</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">textToPrint</span><span class="o">=</span><span class="n">textToPrint</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">BG</span><span class="p">,</span><span class="s1">&#39;[&#39;</span><span class="o">+</span><span class="n">BG</span><span class="p">)</span>
        <span class="n">textToPrint</span><span class="o">=</span><span class="n">textToPrint</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">DE</span><span class="p">,</span><span class="s1">&#39;]&#39;</span><span class="o">+</span><span class="n">DE</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">textToPrint</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">IntSlider</span>    

<span class="c1">#establish interactivity</span>
<span class="n">interact</span><span class="p">(</span><span class="n">generateMadlib</span><span class="p">,</span> <span class="n">currentColumn</span><span class="o">=</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "4e2d98541bd44c439ae032cc80c15123"}
</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.generateMadlib(currentColumn)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="what-to-make-of-the-madlib">
<h2>What to make of the Madlib<a class="headerlink" href="#what-to-make-of-the-madlib" title="Permalink to this headline">¶</a></h2>
<p>What you should take from the above paragraphs is that move to make use comparisons between the modalities is well founded and that the specific features that have been associated across the modalities are justifiably related.  This has been done to support the practice of leveraging insights from one of these modalities to help us learn about another modality.</p>
</div>
<div class="section" id="at-the-heart-of-it-all">
<h2>At the heart of it all<a class="headerlink" href="#at-the-heart-of-it-all" title="Permalink to this headline">¶</a></h2>
<p>Overall, what this guide is designed to do is to take an individual who is presumed to have little familarity with white matter or white matter segmentations and bring them to a point where they have mastered the “meta-ontology” associated with the practice of white matter segmentation. In essence, “mastery of the meta-ontology” is the state an expert in any given field has acheived when they possess a deep familarity with the various approaches and frameworks particular to their field <em>and</em> the systematicly structured relations between those approaches.  Acheiving this is obviously no easy feat.  The strategy of this lesson set is to reinforce users’ intuitions regarding ontological systems <em>that they are already implicitly experts of</em> , and use those as a bridge to understanding ontological systems that were initially utterly mysterious (i.e. digital white matter segmentation).</p>
<p>We are beginning with digital photographs because the kinds of “implicit features of interest” and “implicit categories” that one encounters in this domain are the things that are brains are hardwired to pickout–we are all natural masters of this ability.  As it was posed above, the digital photography example was roughly related to the field of machine-vision, but at it’s heart that field (machine-vision) is attempting to replicate the inherent capability of the visual system using the framework provided.  As complex as that field of research is, we are all experts at this process, and not only identify a massive range of objects in the world, but we can also switch between frameworks and approaches for identifying and categorizing them.  Unfortunately, there are no innate human cognitive systems (specific) for segmenting white matter.  None the less, our goal with this guide is to help you, the user, become familiar with the processes, contexts, and discusssion of digital white matter segmentation, in much the same way that you are capable of identifying objects in the world.  Rather than rely on your intuitions though, we’ll be doing this scientifically, and going through the quantitaive way of doing this. Before embarking on our journey though, we should probably take this opportunity to cash out what is meant by an ontology or meta-ontology.  These concepts will become quite important as we consider some of the more abstract fundamentals of white matter segmentation.</p>
<div class="section" id="what-s-a-meta-ontology-for-that-matter-what-s-an-ontology">
<h3>What’s a meta-ontology?  For that matter, what’s an ontology?<a class="headerlink" href="#what-s-a-meta-ontology-for-that-matter-what-s-an-ontology" title="Permalink to this headline">¶</a></h3>
<p>For our purposes here, we are considering the notion of an ontology in the sense used by <a class="reference external" href="https://en.wikipedia.org/wiki/Ontology_(information_science)">information science</a>.  Fundamental to the process of segmentation (and thus to this guide) is the identification and delineation of differences within members of a larger set.  Repeatedly, we will find that our main curiosity is <em>how ought we go about systematically assigning particular entities we encounter to (presumably) meaningful and useful categories</em>.  There can be no answer to this question without the the provision or specification of an <em>ontology</em>.  Quite simply: we can’t separate objects if we don’t know the kinds of categories they can be separated in to.</p>
<p>For us, in the following lesson sets, the “provision or specification of an ontology” will entail the (either explicit or implicit) <a class="reference external" href="https://en.wikipedia.org/wiki/Operationalization">operationalization</a> of certian components that are common to all ontologies in a fashon that is specific to the current ontology.  Those components are outlined in the following table (<a class="reference external" href="https://en.wikipedia.org/wiki/Ontology_components">adapted from wikipedia</a>):</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Ontology component</strong></p></th>
<th class="head"><p><strong>Rough definition</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Individuals</em></p></td>
<td><p>Those entities which “exist” and are submitted for assignment to “Classes”</p></td>
</tr>
<tr class="row-odd"><td><p><em>Classes</em></p></td>
<td><p>Meaningful/non-arbitrary groupings of <em>Individuals</em> (e.g. labels, categories, etc.</p></td>
</tr>
<tr class="row-even"><td><p><em>Attributes</em></p></td>
<td><p>Properties or characteristics that <em>Individuals</em> or <em>Classes</em> can have</p></td>
</tr>
<tr class="row-odd"><td><p><em>Relations</em></p></td>
<td><p>Those manners or dimensions with which <em>Individuals</em> and <em>Classes</em> can be compared or related</p></td>
</tr>
</tbody>
</table>
<p>This is, admittedly, a very abstract preview of how we will be making use of ontologies throughout our lessons.  Interestingly though, the structured analogy provided in the analogy table <em>already provides</em> specifications for several characteristics.  As it turns out, by decomposing the salient characteristics of the imaging modalities we’ll be considering, we appear to have “carved nature at its joints” (to borrow from Plato) and thereby aligned those modalities along their ontologically salient dimensions.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Ontology component</strong></p></th>
<th class="head"><p><strong>Rough definition</strong></p></th>
<th class="head"><p><strong>Modality Representation Characteristic</strong></p></th>
<th class="head"><p><strong>Modality Source Characteristic</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>Individuals</em></p></td>
<td><p>Those entities which “exist” and are submitted for assignment to <em>Classes</em></p></td>
<td><p><em>Data “atoms”</em></p></td>
<td><p>Atom-object correspondence</p></td>
</tr>
<tr class="row-odd"><td><p><em>Classes</em></p></td>
<td><p>The various meaningful/non-arbitrary groupings available for <em>Individuals</em> (e.g. labels, categories, etc.)</p></td>
<td><p><strong>Labeling schemas / segmentations</strong></p></td>
<td><p><em>Implicit categories</em></p></td>
</tr>
<tr class="row-even"><td><p><em>Attributes</em></p></td>
<td><p>Properties or characteristics that <em>Individuals</em> or <em>Classes</em> can have</p></td>
<td><p><em>Data “atom” content</em></p></td>
<td><p>[causal properties]</p></td>
</tr>
<tr class="row-odd"><td><p><em>Relations</em></p></td>
<td><p>Those manners or dimensions with which <em>Individuals</em> and <em>Classes</em> can be compared or related</p></td>
<td><p>[for us to characterize/discover]</p></td>
<td><p><em>Implicit features of interest</em></p></td>
</tr>
</tbody>
</table>
<p>Thus, any time we apply a segmentation what we’ll be doing is systematically providing rules based on <em>Relations</em> using the <em>Attributes</em> (i.e. quantative characteristics) of the various <em>Individuals</em> in order to determine which <em>Classes</em> the <em>Individuals</em> belong to.</p>
</div>
<div class="section" id="a-second-insight-from-the-analogy-framework-the-role-of-representation">
<h3>A second insight from the analogy framework: the role of representation<a class="headerlink" href="#a-second-insight-from-the-analogy-framework-the-role-of-representation" title="Permalink to this headline">¶</a></h3>
<p>The decomposition in the earlier analogy table also offers another interesting insight.  Each data modailty we are considering is a way of systematically <em>representing</em> something in the world.  Because the relations of the various aspects of the data modalities is preserved by the structure of the analogy posed, it may also be possible to provide a general account of how each of the relevant characteristics from the analogy table helps to instantiate a representation relation with the associated data object.  This may seem like an abstract concern, but (for reasons that will become aparent throughout this endeavor) it is extremely important to clearly and from the outset nail down what it means for the categories and objects we identify in our data to “represent” things in the world.</p>
<p>Explicitly, each <em>Data Token</em> is taken to represent a “real thing in the world” (i.e. <em>Object represented</em> ).  That <em>Data Token</em> is generated by the <em>Source system</em> , which systematically measures the <em>Source phenomena</em> , one of several <em>Attributes</em> possessed by the <em>Object represented</em> . The <em>Data Token</em> preserves information related to the <em>Property of interest</em> that can be used to ascribe the desired <em>Classes</em> to the <em>Individuals</em> of the <em>Data Token</em> .  Because the <em>Data Token</em> “preserves the relevant information” (and in doing so, instantiates the representation relation with the “real thing in the world”), the <em>Classes</em> ascribed to the <em>Data Token</em>’s <em>Individuals</em> can be back-mapped back on to the <em>Object represented</em>.  Indeed, though provided as an account of representation for the modalities in the analogy table, its possible that this could serve as an account of representation more generally.</p>
<p>The above paragraph also just so happens to highlight (quite abstractly) why we care about the various image modalities covered by the analogy table.</p>
</div>
</div>
<div class="section" id="why-we-care-about-image-representations-generally-and-specifically">
<h2>Why we care about image representations generally… and specifically<a class="headerlink" href="#why-we-care-about-image-representations-generally-and-specifically" title="Permalink to this headline">¶</a></h2>
<p>Inherent in the account of representation above is the fact that we can computationally leverage digital images (in any of their various forms) to label their individual components in accordance with whatever goals we may have.  Indeed, this is precisely what our brains do in order to help us interact with the world: they carve the world (visual and otherwise experienced) into categories and particular instances of those categories so that we can make use of that information to guide our behavior.  However, digital images and the algorithms we apply to them increasingly (as our computational and algorithmic technologies advance) have three distinct advantages:  their formal reliability, their processing speed, and their transparency (at least in the case of non black box algorithms).  So long as we can formulate a sensible set of formal rules to apply to the elements of digital images, we can apply them, with great speed and reliabilty, to obtain far more nuanced categorizations that we are able to when using our natural sensory capabilties.  But therein lies the rub: “So long as we can formulate a sensible set of formal rules”–how can we do this?  This is at the heart of our difficulties with image labeling algorithms generally, and our specific goal of performing white matter segmentations.</p>
<p>Because we are using computational approches, our ability to pose formal rules is fairly decent.  Indeed, there are those who have formalized computational languages <em>specific</em> to white matter segmentation (i.e. <a class="reference external" href="https://dx.doi.org/10.1007%2Fs00429-015-1179-4">WMQL: White Matter Query Language</a>) to develop the ability to pose formal white matter segmentation rules as robustly as possible.  However, it has yet to be proven that any particular of these approaches is sufficiently versitile to delineate any specific white matter structure we could possibly be interested in.  It could be that there are some structures–known or unknown–that existing methods are insufficient for delineating.  To use a linguistic analogy, this would be as if a language lacked the formal ability to pose certian propositions.  Moreover, in the case of white matter, we’re probably being hindered by simply <em>not knowing</em> the full range of structures that a formal white matter language would need to be able to express.   To get at the crux of the matter though, such endeavors are merely the provision of a <a class="reference external" href="https://en.wikipedia.org/wiki/Syntax">syntax</a> for describing white matter, and syntax (a formal system of rules for making well formed expressions) alone isn’t enough if our goal is to systematically link our <em>Data Tokens</em> to “objects in the world” and thus be useful and valid representations.  What they lack is a <a class="reference external" href="https://en.wikipedia.org/wiki/Semantics">semantics</a> for describing white matter–a systematic method for mapping meaning (and thus by necessity, representation) to the various components of our data of interest.  It isn’t at all immediately clear how one could go about this, but this lesson set, as a whole, is offered up as a rough attempt at this.</p>
</div>
<div class="section" id="a-rough-attempt-at-a-semantics-for-digital-white-matter">
<h2>A rough attempt at a semantics for digital white matter<a class="headerlink" href="#a-rough-attempt-at-a-semantics-for-digital-white-matter" title="Permalink to this headline">¶</a></h2>
<p>Traditionally, one of the greatest challenges for naturalistic accounts of representation of mental content was finding a compelling account of the link between the somewhat ineffeffable notion of mental content and its ostensible relationship with objects in the world.  One of the more prevalent theories offered was that of <a class="reference external" href="https://plato.stanford.edu/entries/content-teleological/">teleosemantics</a> which endeavored to explain the phenomenon of mental representation in terms of inferred functions or purposes.  This paradigm is subject various forms of rebuttal due to the limitations of our ability to infer the “true” function of a given entity posessing representational properties.  However, this isn’t the case for digital models of white matter.  The “mental content” that we are dealign with is simply formalized data structures.  Because we have developed the imaging devices and the algorithms that trandsuce the measurements of the world into data objects and derivative models thereof, we don’t need to infer the structure of that the process which resulted in the “information token” (i.e. representation).  Rather, what we need is a clear, comprehensive, and compelling account of the processes which reliably give rise to valid models of coherent white matter tracts.</p>
<p>Our task with this guide is to provide a robust account of how the we can systematically modify and transform the representational elements available to us in tractograms in such a way that carve digital models of white matter into what we take to be the meaningful subcomponents of the white matter.  This means identifying, tracing, and describing the elementary methods used to compose, describe, and delineate a given white matter structure from amongst the other candidate components (i.e. streamlines) of a tractogram.  To do this we will need to consider <em>all</em> of the information that we can make use of–both explicit and implicit–from our various available data objects, and what steps we can take to select a specific and coherent structure of interest.</p>
</div>
<div class="section" id="the-grand-insight">
<h2>The grand insight<a class="headerlink" href="#the-grand-insight" title="Permalink to this headline">¶</a></h2>
<p>The overarching insight that this guide will instill, both within the context of digital white matter segmentation and within the context of representation more generally, is somewhat counterintuitive.  <strong>In practice, the way we form a representation is by applying a (potentially large and complicated) sequence of <em>exclusion</em> conditions to some set of vandidates</strong>.  Intuitively, we might expect that we would be trying to identify the “essential features” of our category of interest, but that’s not really a viable tactic, from a practical standpoint.  Instead, what we’ll see is that what we’re actually doing with white matter segmentation (and perhaps representation generally) is applying an (ideally finite) set of rules to a data object to <em>exclude</em> everything <em>except</em> the specific class (or token) we are interested in.  In providing a semantics–a systematic account of how to link a data token to a (presumably) “real world” category–we will be providing a comprehensive (to the extent that we are able) account of the tactics that can be used to eliminate data/representation elements that are inconsistent with salient properties of the “real world” class.  This is what is entailed by a semantics of white matter, and hopefully it is what you will find in the following chapters.</p>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"8e28001de55c4922859a5790b7582df4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a49f79f78bcf4ef9af06e3391ea5e11c": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "eec97e9a05f7481e9cb917f452817146": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": false, "description": "currentColumn", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_8e28001de55c4922859a5790b7582df4", "max": 4, "min": 0, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_a49f79f78bcf4ef9af06e3391ea5e11c", "value": 0}}, "be5fb1fcb244461d9491c923897ab93c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4e2d98541bd44c439ae032cc80c15123": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_eec97e9a05f7481e9cb917f452817146", "IPY_MODEL_f8d632a0c6514459b8d65be942000411"], "layout": "IPY_MODEL_be5fb1fcb244461d9491c923897ab93c"}}, "fd228fe8572f458a8f31c15faccdba08": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f8d632a0c6514459b8d65be942000411": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_fd228fe8572f458a8f31c15faccdba08", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "A fill-in-the-blank description of [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m :\n\nWhen we engage in research using [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m we are interested in learning more about a particular [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0m.  However, in order to study the given [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0m effectively, we need to preserve information (i.e. observations) about it for later analysis.  In the case of [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m we typically use a(n) [\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0m to obtain data about a [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0m. It is worth noting though, that our decision to use a [\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0m is based upon (1) the availability of viable alternatives and (2) the general property of the [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0m (e.g. [\u001b[1m\u001b[95m\u001b[4mProperty of interest]\u001b[0m\u001b[0m\u001b[0m) that we are interested in, or whatever more specific goals we may have.  Indeed, the kinds of scientific questions we can ask about [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0ms  (or  [\u001b[1m\u001b[95m\u001b[4mImplicit features of interest]\u001b[0m\u001b[0m\u001b[0m, or even [\u001b[1m\u001b[95m\u001b[4mImplicit categories]\u001b[0m\u001b[0m\u001b[0m) using [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m are delimited by the technical limitations of the[\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0ms used, and so to then, are the general kinds of things that end up getting studied/characterized using [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m.  Resulting chicken-or-the-egg conundrums and streetlight effects notwithstanding, the ability to develop insights about [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0ms using information about [\u001b[1m\u001b[95m\u001b[4mProperty of interest]\u001b[0m\u001b[0m\u001b[0m derived from [\u001b[1m\u001b[95m\u001b[4mSource phenomena]\u001b[0m\u001b[0m\u001b[0m has, in practice, proven to be useful for identifying [\u001b[1m\u001b[95m\u001b[4mImplicit features of interest]\u001b[0m\u001b[0m\u001b[0m, and (by extension) [\u001b[1m\u001b[95m\u001b[4mImplicit categories]\u001b[0m\u001b[0m\u001b[0m, both in specific [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0ms and in [\u001b[1m\u001b[95m\u001b[4mObject represented]\u001b[0m\u001b[0m\u001b[0ms generally.  Lets briefly consider how the data objects used in [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m facilitate this.\n\n  The scientific utility of [\u001b[1m\u001b[95m\u001b[4mGeneral imaging modality]\u001b[0m\u001b[0m\u001b[0m is underwritten by our ability to store, access, and modify information contained within the standard data object used in this disipline, the [\u001b[1m\u001b[95m\u001b[4mData token]\u001b[0m\u001b[0m\u001b[0m.  In each of these data objects information that was produced by a [\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0m measuring [\u001b[1m\u001b[95m\u001b[4mSource phenomena]\u001b[0m\u001b[0m\u001b[0m is stored in a structured fashion.  Specifically, in the case of a [\u001b[1m\u001b[95m\u001b[4mData token]\u001b[0m\u001b[0m\u001b[0m, that information is stored in a [\u001b[1m\u001b[95m\u001b[4mData dimensionality]\u001b[0m\u001b[0m\u001b[0m structure wherein each [\u001b[1m\u001b[95m\u001b[4mData atom]\u001b[0m\u001b[0m\u001b[0m contains a [\u001b[1m\u001b[95m\u001b[4mData atom content]\u001b[0m\u001b[0m\u001b[0m.  Each of these [\u001b[1m\u001b[95m\u001b[4mData atom]\u001b[0m\u001b[0m\u001b[0ms stores information about [\u001b[1m\u001b[95m\u001b[4mSource phenomena]\u001b[0m\u001b[0m\u001b[0m as measured by the source [\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0m and, in doing so, represents a(n) [\u001b[1m\u001b[95m\u001b[4mAtom-object correspondence]\u001b[0m\u001b[0m\u001b[0m.  Typically, a [\u001b[1m\u001b[95m\u001b[4mData token]\u001b[0m\u001b[0m\u001b[0m contains a(n) [\u001b[1m\u001b[95m\u001b[4mMetadata]\u001b[0m\u001b[0m\u001b[0m component which stores information about the source [\u001b[1m\u001b[95m\u001b[4mSource system]\u001b[0m\u001b[0m\u001b[0m or other information relevant to (but not derivable from) the data object itself.  These files/data objects are typically associated with file extensions like [\u001b[1m\u001b[95m\u001b[4mFile extension]\u001b[0m\u001b[0m\u001b[0m and are about [\u001b[1m\u001b[95m\u001b[4mData size]\u001b[0m\u001b[0m\u001b[0m in size, which can give you some sense of the number of [\u001b[1m\u001b[95m\u001b[4mData atom content]\u001b[0m\u001b[0m\u001b[0ms the object contains.\n"}]}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="../chapterSummaries.html" title="previous page">White Matter Segmentation Education - WiMSE</a>
    <a class='right-next' id="next-link" href="Intro_to_discretized_image_representation_%26_maps.html" title="next page">Introduction to discritized image representation and maps</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Daniel Bullock<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>
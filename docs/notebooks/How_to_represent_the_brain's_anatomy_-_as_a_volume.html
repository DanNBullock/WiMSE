
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>How to represent the brain’s anatomy - as a volume &#8212; White Matter Segmentation Education (WiMSE)</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to interpret a volumetric brain segmentation" href="How_to_interpret_a_volumetric_brain_segmentation.html" />
    <link rel="prev" title="A consideration of jpegs and brain images" href="A_consideration_of_jpegs_and_brain_images.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  
  <h1 class="site-logo" id="site-title">White Matter Segmentation Education (WiMSE)</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../landingPage.html">
   White Matter Segmentation Education (WiMSE)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Front matter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Author_and_funding_information.html">
   Author and Funding information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../chapterSummaries.html">
   White Matter Segmentation Education - WiMSE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Building_Bridges.html">
   Building bridges from the familiar to the unfamiliar
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Building intuitions with digital images
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro_to_discretized_image_representation_%26_maps.html">
   Introduction to discretized image representation and maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Aligning_two_images.html">
   Aligning two images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Multi_object_maps_in_images.html">
   Multi object maps in images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="A_consideration_of_jpegs_and_brain_images.html">
   A consideration of jpegs and brain images
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Working with NIfTI data
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   How to represent the brain’s anatomy - as a volume
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="How_to_interpret_a_volumetric_brain_segmentation.html">
   How to interpret a volumetric brain segmentation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  White matter and tractography
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="The_voxel_and_the_streamline.html">
   The voxel and the streamline
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="The_source_tractogram.html">
   The source tractogram
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Segmenting tractography
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="A_first_segmentation.html">
   A first segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Biological_Plausibility_for_Tractograms.html">
   Biological Plausibility for Tractograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ROIs_as_tools.html">
   ROIs as tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Using_ROIs_as_tools.html">
   <em>
    Using
   </em>
   ROIs as tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="A_second_segmentation-categorical_segmentation.html">
   A second segmentation - categorical segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Advanced_anatomically-based_segmentation.html">
   Advanced anatomically-based segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Example_Segmentation-uncinate_fasiculus.html">
   Example Segmentation: uncinate fasciculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Example_Segmentation-IFOF.html">
   Example Segmentation: inferior fronto-occipital fasciculus (IFOF)
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Epilogue
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Closing_Thoughts.html">
   Closing Thoughts
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/How_to_represent_the_brain's_anatomy_-_as_a_volume.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/DanNBullock/WiMSE/edit/master/notebooks/How_to_represent_the_brain's_anatomy_-_as_a_volume.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/DanNBullock/WiMSE/master?urlpath=tree/notebooks/How_to_represent_the_brain's_anatomy_-_as_a_volume.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="how-to-represent-the-brain-s-anatomy-as-a-volume">
<h1>How to represent the brain’s anatomy - as a volume<a class="headerlink" href="#how-to-represent-the-brain-s-anatomy-as-a-volume" title="Permalink to this headline">¶</a></h1>
<p>Not unlike the way that the satellite images of the previous chapters were a “picture” of the world, a <a class="reference external" href="https://en.wikipedia.org/wiki/Magnetic_resonance_imaging#T1_and_T2">T1 image</a> can be thought of as a “picture” of a brain in a similar fashion.  The specifics of <a class="reference external" href="https://en.wikipedia.org/wiki/Physics_of_magnetic_resonance_imaging">the physics underlying this process</a> are beyond the scope of this lesson book.  For now let us keep the analogy of the 2-D digital image in mind as we move into a consideration of the NIfTI itself.</p>
<p>Let’s begin by loading it into memory and considering its data size.  Whereas previously we were working with .jpg and .png files, we will now be working with .nifti files, which are typically stored as .nii.gz, which corresponds to a compressed state.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#this code ensures that we can navigate the WiMSE repo across multiple systems</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#get top directory path of the current git repository, under the presumption that </span>
<span class="c1">#the notebook was launched from within the repo directory</span>
<span class="n">gitRepoPath</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s1">&#39;git&#39;</span><span class="p">,</span> <span class="s1">&#39;rev-parse&#39;</span><span class="p">,</span> <span class="s1">&#39;--show-toplevel&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1">#move to the top of the directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">gitRepoPath</span><span class="p">)</span>

<span class="c1">#set path to T1</span>
<span class="n">t1Path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gitRepoPath</span><span class="p">,</span><span class="s1">&#39;exampleData&#39;</span><span class="p">,</span><span class="s1">&#39;t1.nii.gz&#39;</span><span class="p">)</span>
<span class="c1">#obtain file information about the t1 file</span>
<span class="n">T1fileinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">t1Path</span><span class="p">)</span>

<span class="c1">#print out some of the ifo</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;T1 size (in bytes)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">T1fileinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>T1 size (in bytes)
5688165
</pre></div>
</div>
</div>
</div>
<p>The T1 NIfTI image we just loaded is about 3.6 MB in size, which is roughly close to a high resolution digital image file.  Given what we know about digital image (every pixel has 3 to 4 <em>integer</em> values which correspond to its color characteristics), we can get a rough sense about the number data entries contained within a NIfTI, and–by extension–the general amount of “information” (a complex topic we won’t go into here) that a NIfTI image contains about volume of space it is represents.</p>
<p>Quite helpfully, the NIfTI data type features a “header” (described in detail <a class="reference external" href="https://brainder.org/2012/09/23/the-nifti-file-format/">here</a> and <a class="reference external" href="https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h">here</a>) which contains a great deal of <a class="reference external" href="https://en.wikipedia.org/wiki/Metadata">metadata</a> about the associated image data.  Think of it as being analogous to the <a class="reference external" href="https://en.wikipedia.org/wiki/Exif">Exif</a> data of a typical digital image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#begin process of loading file as a T1 nifti using nibabel</span>
<span class="kn">import</span> <span class="nn">nibabel</span> <span class="k">as</span> <span class="nn">nib</span>
<span class="c1">#import the data</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">nib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">t1Path</span><span class="p">)</span>

<span class="c1">#extract the header info</span>
<span class="n">T1header</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">header</span>
<span class="c1">#print the output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">T1header</span><span class="p">)</span>  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;nibabel.nifti1.Nifti1Header&#39;&gt; object, endian=&#39;&lt;&#39;
sizeof_hdr      : 348
data_type       : b&#39;&#39;
db_name         : b&#39;&#39;
extents         : 0
session_error   : 0
regular         : b&#39;r&#39;
dim_info        : 0
dim             : [  3 182 218 182   1   1   1   1]
intent_p1       : 0.0
intent_p2       : 0.0
intent_p3       : 0.0
intent_code     : none
datatype        : int16
bitpix          : 16
slice_start     : 0
pixdim          : [-1.   1.   1.   1.   1.8  0.   0.   0. ]
vox_offset      : 0.0
scl_slope       : nan
scl_inter       : nan
slice_end       : 0
slice_code      : unknown
xyzt_units      : 10
cal_max         : 0.0
cal_min         : 0.0
slice_duration  : 0.0
toffset         : 0.0
glmax           : 0
glmin           : 0
descrip         : b&#39;FSL5.0&#39;
aux_file        : b&#39;&#39;
qform_code      : mni
sform_code      : mni
quatern_b       : 0.0
quatern_c       : 1.0
quatern_d       : 0.0
qoffset_x       : 90.0
qoffset_y       : -126.0
qoffset_z       : -72.0
srow_x          : [-1.  0.  0. 90.]
srow_y          : [   0.    1.    0. -126.]
srow_z          : [  0.   0.   1. -72.]
intent_name     : b&#39;&#39;
magic           : b&#39;n+1&#39;
</pre></div>
</div>
</div>
</div>
<p>That’s a lot of information!</p>
<p>For now, we’ll only briefly consider a few of these features, but we’ll get to each of these as necessary as we proceed through the next set of lessons.</p>
<p><a class="reference external" href="https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/dim.html/document_view"><strong>dim</strong></a>:  This field displays the dimensions of the NIfTI image data object.  The first number (3, in this case) indicates the total number of dimensions occupied by the data.  The next several values (as many as indicated by the first number in this vector) correspond to the span of each of those dimensions.  Thus, in this case the first dimension spans 145 entries, the second spans 174 entries, and the third spans 145 entries.</p>
<p><a class="reference external" href="https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/datatype.html"><strong>datatype</strong></a>:  This field corresponds to the <a class="reference external" href="https://en.wikipedia.org/wiki/Computer_number_format"><em>type</em> of numerical data</a>  contained within each of the nifti image data object’s entries.  Here we see that it is “float32”.  This gives us two primary pieces of information: (1) we now know that the numerical entries are <em>not</em> integers or whole numbers (and thus they can adopt values <em>between</em> 1,2,3, etc.)  and (2) we now know that each of these entries <a class="reference external" href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">takes up about 4 bytes</a>.</p>
<p><a class="reference external" href="https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/pixdim.html/document_view"><strong>pixdim</strong></a>:  Ignoring the initial (-1) and last four values in this field (1.8 0. 0. 0.), we see the numbers 1. 1. 1..   These indicate the size of the dimensions of real world space represented by the NIfTI image data object’s entries.  In the case of the map from the previous chapter, this would have corresponded roughly to hundreds of square miles (though the exact quantity would be a complex issue, due to <a class="reference external" href="https://en.wikipedia.org/wiki/Map_projection">projection issues</a>).  In this case, we see that each of these values is 1 millimeter, which means that the data in any entry of the NIfTI represents a 1 by 1 by 1 for a total of 1 cubic millimeter volume of space.  The fifth value (1) isn’t particularly relevant to this particular nifti, as our nifti data object is only 3 dimensional.  If we were examining fMRI data, this value would indicate the time-step used for the data.</p>
<p>Also, it’s important to note that all three of the spatial dimension measures examined above are the same.  This need not be the case, and it could be that our voxels are actually representing units of space whose sides are not all equal.  In this case though, we can say that the voxels are “isometric”, meaning that all faces are the same size.</p>
<p><a class="reference external" href="https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/qsform.html"><strong>qform_code</strong></a>:  This field specifies the orientation schema that will be detailed in subsequent fields.  In the previous lesson we considered the intersection of the <a class="reference external" href="https://en.wikipedia.org/wiki/IERS_Reference_Meridian">prime meridian</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Equator">equator</a>, also known <a class="reference external" href="https://en.wikipedia.org/wiki/Null_Island">“Null Island”</a>, as the “origin” of the major orientation schema we used, and endeavored to overlay these locations when we aligned global satellite images.  But imagine that we instead used some other coordinate system to establish a different origin point.  Alternatively, what if we stopped using degrees as our unit of measure or that we flipped our labeling convention for east and west? In any of these cases, we would need to find a way to indicate what our reference frame was.  This need for specification of orientation system is reflected in the <strong>qform_code</strong> of the NIfTI header.  Here, we have some indicators (“mni” or “talarach”, for example) that establish a common reference frame for orienting the image data.  “mni” and “talarach” respectively correspond to the standard Montreal Neurological Institute (<a class="reference external" href="https://doi.org/10.1002/hbm.460030304">Collins et al., 1995</a>) and Talairach <a class="reference external" href="https://www.semanticscholar.org/paper/Co-Planar-Stereotaxic-Atlas-of-the-Human-Brain%3A-An-Talairach-Tournoux/f2c1ddf5c34a5899c5cbee5b5dc469f39ff9c21e">Talairach and Tournoux, 1988</a> atlases.  These and other neuroimaging reference frames are well described elsewhere (e.g. <a class="reference external" href="https://www.lead-dbs.org/about-the-mni-spaces/">here</a> and <a class="reference external" href="http://www.fieldtriptoolbox.org/faq/how_are_the_different_head_and_mri_coordinate_systems_defined/">here</a>).</p>
<p><a class="reference external" href="https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/qsform.html"><strong>qoffset_x, qoffset_y , qoffset_z</strong></a> (see “METHOD 2”):  These numbers are the transforms necessary to align this image to the reference specified in “qform_code”.  Referring back to our previous lessons, you can think of these numbers as the shift needed to align the political map with the geographic map. Now though, we have three dimensions (as opposed to two) and need to align each of them.  Typically our goals when aligning MRI images  are threefold:  (1) that the images share matching scales, (2) that the images exhibit the same orientation (e.g. top -&gt; top, left -&gt; left, etc.), and (3) that the images points share the same “origin”.</p>
<p>We have now looked at some important features of the <a class="reference external" href="https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h">NIfTI header</a>.  However, we haven’t yet taken a look at the actual data component of a NIfTI, as we did with digital images in previous lessons.  Let’s move forward and take a look at this now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Data dimensions&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(How many data entries are spanned across each dimension)&#39;</span><span class="p">)</span>
<span class="c1">#extract data dimensions and store in DataDimensions variable</span>
<span class="n">dataDimensions</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataDimensions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data dimensions
(How many data entries are spanned across each dimension)
(182, 218, 182)
</pre></div>
</div>
</div>
</div>
<p>Here we are no longer indexing into the header, but we nonetheless see that the size of the image data is (182,218,182), which is consistent with the information we obtained from the “dim” field of the header.  In theory, the metadata contained should always be consistent with the actual data, but it is possible for these to become inconsistent due to improper file generation or manipulation.</p>
<p>It’s also worth noting that the <a class="reference external" href="https://nipy.org/nibabel/">nibabel</a> <a class="reference external" href="https://doi.org/10.5281/zenodo.4295521">python software package</a> offers a number of methods for extracting <em>specific</em> information from the header.  For example, we can extract the information specific to the spatial dimensions of the image we are looking at.  This capability can be helpful when performing a series of computations or developing your own sets of functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Voxel dimensions for T1 (in mm)&#39;</span><span class="p">)</span>
<span class="c1">#extract dimensions and store in voxelDims variable</span>
<span class="n">voxelDims</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">header</span><span class="o">.</span><span class="n">get_zooms</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">voxelDims</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Voxel dimensions for T1 (in mm)
(1.0, 1.0, 1.0)
</pre></div>
</div>
</div>
</div>
<p>Here, we obtain a result that matches what we saw by looking at the entire header.  Now though we have stored this specific information in a variable.  Now, by taking the size of an individual voxel and the dimensions of the entire data block we can compute the amount of space that this NIfTI object represents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;How much total space does this representation correspond to?&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;{dataDimensions[0]*voxelDims[0]} mm by {dataDimensions[1]*voxelDims[1]} mm by {dataDimensions[2]*voxelDims[2]} mm&#39;</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>How much total space does this representation correspond to?

182.0 mm by 218.0 mm by 182.0 mm
</pre></div>
</div>
</div>
</div>
<p>For those more comfortable with imperial measurements, this corresponds to a volume of space that is slightly more than half a foot wide (~.6 feet), almost three quarters of a foot tall (~.71 feet), and slightly more than half a foot deep (~.6 feet).  This information gives us a sense of the gross scale of the data contained within a NIfTI.  But what about the data contained within a specific entry?  What’s in a voxel?</p>
<p>Let’s actually index into a specific voxel and see what information is stored there.  From the header, we know that it will be a float value, but we don’t, as of yet, have a ballpark sense of what this value will be.  We’ll just arbitrarily pick a value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#extract the actual data into the variable data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">get_fdata</span><span class="p">()</span>
<span class="c1">#arbitrarily index into the item the 96 X, 86 Y, 57 Z location and read it out</span>
<span class="n">data</span><span class="p">[</span><span class="mi">96</span><span class="p">,</span><span class="mi">86</span><span class="p">,</span><span class="mi">57</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>235.0
</pre></div>
</div>
</div>
</div>
<p>235.0.  We don’t really know what this number means, whether it is a lot or a little, or if it is unusual, but it at least gives us some sense of the kind of numbers this NIfTI object contains.  What about all of the others though?  How can we get a comprehensive and informative sense of those?  Let’s take a hint from our digital image lessons and explore this NIfTI data by plotting a histogram of the data values for each voxel.  In a certain sense, we’re lucky (relative to the digital image case) because we can plot a sensible histogram for this information, as there aren’t 3 distinct color channels to worry about (which would make it difficult to interpret a histogram or set of histograms).  Additionally, we can get a sense of how many total voxels there are, how many are “non-zero” (in that they contain a measure distinct from zero), and what proportion of the total number of voxels this corresponds to. Let’s do that now.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1">#necessary for plotting</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="c1">#transform data array for use</span>
<span class="n">unwrappedData</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total number of voxels&#39;</span><span class="p">)</span>
<span class="c1">#compute total number of voxels via straightforward multiplication</span>
<span class="n">voxTotal</span><span class="o">=</span><span class="n">dataDimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">dataDimensions</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">dataDimensions</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">voxTotal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Minimum voxel value&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">unwrappedData</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Maximum voxel value&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">unwrappedData</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1">#set value at which to split the data</span>
<span class="c1">#Operating under the assumption that negative values are not viable</span>
<span class="n">splitPoint</span><span class="o">=</span><span class="mi">0</span>

<span class="c1">#define two functions for obtaining boolean vectors for numerical comparison</span>
<span class="k">def</span> <span class="nf">smallVal</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">n</span><span class="o">&lt;=</span><span class="n">splitPoint</span>
<span class="k">def</span> <span class="nf">largeVal</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">n</span><span class="o">&gt;</span><span class="n">splitPoint</span>

<span class="c1">#apply the function to the unwrapped data</span>
<span class="n">result</span><span class="o">=</span><span class="nb">map</span><span class="p">(</span><span class="n">smallVal</span><span class="p">,</span><span class="n">unwrappedData</span><span class="p">)</span>
<span class="c1">#convert the output to a usable format </span>
<span class="n">smallBool</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1">#apply the function to the unwrapped data</span>
<span class="n">result</span><span class="o">=</span><span class="nb">map</span><span class="p">(</span><span class="n">largeVal</span><span class="p">,</span><span class="n">unwrappedData</span><span class="p">)</span>
<span class="c1">#convert the output to a usable format </span>
<span class="n">largeBool</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of voxel values less than or equal to zero&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">smallBool</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of voxel values greater than zero&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">largeBool</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Proportion of entries greater than zero (i.e. containing data)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">largeBool</span><span class="p">)</span><span class="o">/</span><span class="n">voxTotal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1">#perform plotting of zero/negative values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">unwrappedData</span><span class="p">[</span><span class="n">smallBool</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Voxel Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Voxels&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of voxel less than or equal to zero&#39;</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">18.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">)</span>

<span class="c1">#perform plotting of positive values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">unwrappedData</span><span class="p">[</span><span class="n">largeBool</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Voxel Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Voxels&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of voxel values greater than zero&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of voxels
7221032

Minimum voxel value
-37.0

Maximum voxel value
789.0

Number of voxel values less than or equal to zero
2203910

Number of voxel values greater than zero
5017122

Proportion of entries greater than zero (i.e. containing data)
0.6947929326445306
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Distribution of voxel values greater than zero&#39;)
</pre></div>
</div>
<img alt="../_images/How_to_represent_the_brain's_anatomy_-_as_a_volume_13_2.png" src="../_images/How_to_represent_the_brain's_anatomy_-_as_a_volume_13_2.png" />
</div>
</div>
<p>It seems that about a quarter of the voxels contain data that we would consider to be representative of the brain, with the assumption that “empty” voxels correspond to background and/or uninformative voxels.  Depending on whether or not this T1 has had the brain “extracted” (i.e. the brain isolated from the rest of the head, neck, and body, via a masking process, see <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2004.03.010">Boesen et al. 2004</a> for more), this proportion may also include non-brain tissues.  Note how we had to split the histogram in two.  Had we not done this, the number of empty voxels would have overwhelmed the visualization and we wouldn’t have been able to observe the distribution visible in the plot on the right due to the extreme number of values right below 0.</p>
<p>Now that we have a sense of the numerical variability of the data in this NIfTI, let’s get a sense of how these values are laid out spatially.  Keep in mind that, just like a digital image wherein the i,j entry of the data array represents a portion of space that spatially adjacent to the i,j-1 (or i,j+1, or i+1,j etc.), the i,j,k entry of a NIfTI is spatially adjacent to the i,j,k+1 entry.  We can get a better sense of this by interacting with the NIfTI using a <a class="reference external" href="https://nipy.org/niwidgets/">niwidget</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">niwidgets</span> <span class="k">import</span> <span class="n">NiftiWidget</span>

<span class="n">t1Widget</span> <span class="o">=</span> <span class="n">NiftiWidget</span><span class="p">(</span><span class="n">t1Path</span><span class="p">)</span>

<span class="n">t1Widget</span><span class="o">.</span><span class="n">nifti_plotter</span><span class="p">(</span><span class="n">colormap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 432x288 with 0 Axes&gt;
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "e5b97043305649c798d89c47ffcf3c58", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<p>Take a moment to shift through the NIfTI image plotted above.  If you want a more standard visualization, feel free to switch the colormap to gray.  <strong>As a challenge, try and shift the x, y, and z sliders to cross at the <a class="reference external" href="https://en.wikipedia.org/wiki/Anterior_commissure">anterior commissure</a>, and take note of the coordinates of this point</strong>.  Take note that the left, posterior, inferior corner is the 0 coordinate.  The coordinate shift that results in the anterior commissure being at (0,0,0) occurs after the previously qoffset information has been applied.  After this transform has been applied, locations in the left hemisphere are characterized by coordinates that have a negative first value (x coordinate).  This is because the image is oriented with respect to the <a class="reference external" href="https://www.fieldtriptoolbox.org/faq/how_are_the_different_head_and_mri_coordinate_systems_defined/#details-on-the-acpc-coordinate-system">ACPC reference space</a>.</p>
<p>As an alternative to trying to find the anterior commissure manually, we can also use the information in the header (assuming its accurate) to compute its location in this image data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;T1 voxel resolution (in mm)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">header</span><span class="o">.</span><span class="n">get_zooms</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;T1 voxel affine&#39;</span><span class="p">)</span>
<span class="n">imgAff</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">affine</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">affine</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coordinates of anterior commissure&#39;</span><span class="p">)</span>
<span class="c1">#force absolute value in order to perform the math correctly</span>
<span class="n">imgSpatialTrans</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">([</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">/</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">/</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">/</span><span class="n">imgAff</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">imgSpatialTrans</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>T1 voxel resolution (in mm)
(1.0, 1.0, 1.0)

T1 voxel affine
[[  -1.    0.    0.   90.]
 [   0.    1.    0. -126.]
 [   0.    0.    1.  -72.]
 [   0.    0.    0.    1.]]

Coordinates of anterior commissure
[ 90. 126.  72.]
</pre></div>
</div>
</div>
</div>
<p>If you move the slider coordinates to the value specified under “Coordinates of anterior commissure”, your crosshair should directly target the <a class="reference external" href="https://en.wikipedia.org/wiki/Anterior_commissure">anterior commissure</a>.  However, the signs of the numbers in this affine matrix suggest that something may be afoot.  Let’s consider in a bit more detail what these numbers might be indicating.</p>
<p>Admittedly, the notion of a shift or transform gets a bit more complicated in three dimensions, where it is possible to have your data rotated or flipped along multiple dimensions.  With a standard 2-D image this would have been quite obvious upon inspection–you would notice if the world had been rotated 90% or flipped such that Russia was west of the US’s “east” coast!  Although some of these changes are obvious in a NIfTI image (for example a 90 degree rotation), some are less obvious.  For example, as it turns out, <strong>this brain data is flipped in its x axis!</strong>. Because of this the right hemisphere data is stored in indexes smaller than the X coordinate of the anterior commissure (90) while the left hemisphere data is stored in indexes that are larger than the X coordinate of the anterior commissure.  This is contrary to standard orientation schemas.  Let’s take a look at how we could come to know this by taking a look at the <a class="reference external" href="https://en.wikipedia.org/wiki/Affine_transformation">affine transform matrix</a> again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;T1 voxel affine&#39;</span><span class="p">)</span>
<span class="n">imgAff</span><span class="o">=</span><span class="n">img</span><span class="o">.</span><span class="n">affine</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">affine</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>T1 voxel affine
[[  -1.    0.    0.   90.]
 [   0.    1.    0. -126.]
 [   0.    0.    1.  -72.]
 [   0.    0.    0.    1.]]
</pre></div>
</div>
</div>
</div>
<p>You’ll notice that the upper left hand value (-1.), which specifies the voxel resolution of the Y dimension, is negative.  This is as opposed to the values found for the Y and Z dimension resolutions, which are positive.  Considered in conjunction with the information in the first three rows for the rightmost column (the <strong>qoffset_</strong> information from the header), this indicates that a flip in the X dimension is present.  This is why the <em>np.abs</em> function was used earlier when computing the anterior commissure location (NOTE: this strategy will not work for all affine flip cases, but did work in this case).  For more detailed examinations and considerations of affine transforms for neuroimaging data consider looking at dipy’s <a class="reference external" href="https://www.dipy.org/documentation/1.0.0./examples_built/affine_registration_3d/">documentation on the subject</a>.</p>
<p>Now that we have developed a good sense of the information stored in a T1 NIfTI image, move on to think about how tractography data is different from this.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="A_consideration_of_jpegs_and_brain_images.html" title="previous page">A consideration of jpegs and brain images</a>
    <a class='right-next' id="next-link" href="How_to_interpret_a_volumetric_brain_segmentation.html" title="next page">How to interpret a volumetric brain segmentation</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Daniel Bullock<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>